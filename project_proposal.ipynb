{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(42)#set  random seed for reproducability\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik7brown/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/erik7brown/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Race</th>\n",
       "      <th>Num_Arrests</th>\n",
       "      <th>CensusTract</th>\n",
       "      <th>County</th>\n",
       "      <th>Borough</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024401</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>6408</td>\n",
       "      <td>2755</td>\n",
       "      <td>3653</td>\n",
       "      <td>8.3</td>\n",
       "      <td>87.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>44.6</td>\n",
       "      <td>2703</td>\n",
       "      <td>70.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>36085024800</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4908</td>\n",
       "      <td>2540</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>73.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>36085024800</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4908</td>\n",
       "      <td>2540</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>73.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>36085024800</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4908</td>\n",
       "      <td>2540</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>73.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>36085024800</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4908</td>\n",
       "      <td>2540</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>73.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36085024800</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4908</td>\n",
       "      <td>2540</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>73.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36085024800</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4908</td>\n",
       "      <td>2540</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>73.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024800</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4908</td>\n",
       "      <td>2540</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>73.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024800</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4908</td>\n",
       "      <td>2540</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>73.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36085024800</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4908</td>\n",
       "      <td>2540</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.7</td>\n",
       "      <td>91.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2077</td>\n",
       "      <td>73.6</td>\n",
       "      <td>24.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36081111300</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>2476</td>\n",
       "      <td>1182</td>\n",
       "      <td>1294</td>\n",
       "      <td>11.9</td>\n",
       "      <td>47.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>41.1</td>\n",
       "      <td>1144</td>\n",
       "      <td>80.9</td>\n",
       "      <td>11.5</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36081109300</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>3216</td>\n",
       "      <td>1544</td>\n",
       "      <td>1672</td>\n",
       "      <td>17.3</td>\n",
       "      <td>59.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>1484</td>\n",
       "      <td>83.2</td>\n",
       "      <td>11.6</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36081010100</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>2501</td>\n",
       "      <td>1265</td>\n",
       "      <td>1236</td>\n",
       "      <td>18.8</td>\n",
       "      <td>67.6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>35.3</td>\n",
       "      <td>1465</td>\n",
       "      <td>86.6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11314</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36081099705</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>2720</td>\n",
       "      <td>1418</td>\n",
       "      <td>1302</td>\n",
       "      <td>7.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>36.2</td>\n",
       "      <td>1165</td>\n",
       "      <td>77.9</td>\n",
       "      <td>17.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11315</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36081099705</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>2720</td>\n",
       "      <td>1418</td>\n",
       "      <td>1302</td>\n",
       "      <td>7.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>36.2</td>\n",
       "      <td>1165</td>\n",
       "      <td>77.9</td>\n",
       "      <td>17.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11316</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36081099705</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>2720</td>\n",
       "      <td>1418</td>\n",
       "      <td>1302</td>\n",
       "      <td>7.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>36.2</td>\n",
       "      <td>1165</td>\n",
       "      <td>77.9</td>\n",
       "      <td>17.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11317</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36081099705</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>2720</td>\n",
       "      <td>1418</td>\n",
       "      <td>1302</td>\n",
       "      <td>7.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>36.2</td>\n",
       "      <td>1165</td>\n",
       "      <td>77.9</td>\n",
       "      <td>17.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11318</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>36061019702</td>\n",
       "      <td>New York</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>2127</td>\n",
       "      <td>1101</td>\n",
       "      <td>1026</td>\n",
       "      <td>23.2</td>\n",
       "      <td>32.8</td>\n",
       "      <td>...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>15.6</td>\n",
       "      <td>31.2</td>\n",
       "      <td>1229</td>\n",
       "      <td>76.3</td>\n",
       "      <td>18.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11319</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36005003700</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>246</td>\n",
       "      <td>128</td>\n",
       "      <td>118</td>\n",
       "      <td>57.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>96</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11320</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36005003700</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>246</td>\n",
       "      <td>128</td>\n",
       "      <td>118</td>\n",
       "      <td>57.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>96</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11321</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36005037000</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2463</td>\n",
       "      <td>1099</td>\n",
       "      <td>1364</td>\n",
       "      <td>29.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>1118</td>\n",
       "      <td>77.5</td>\n",
       "      <td>15.3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11322</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36005046000</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2849</td>\n",
       "      <td>1225</td>\n",
       "      <td>1624</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>47.4</td>\n",
       "      <td>989</td>\n",
       "      <td>80.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11323</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36005046000</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2849</td>\n",
       "      <td>1225</td>\n",
       "      <td>1624</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>47.4</td>\n",
       "      <td>989</td>\n",
       "      <td>80.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11324</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>36005046000</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2849</td>\n",
       "      <td>1225</td>\n",
       "      <td>1624</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>47.4</td>\n",
       "      <td>989</td>\n",
       "      <td>80.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11325</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36005046000</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2849</td>\n",
       "      <td>1225</td>\n",
       "      <td>1624</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>47.4</td>\n",
       "      <td>989</td>\n",
       "      <td>80.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11326</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>36005038800</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2471</td>\n",
       "      <td>953</td>\n",
       "      <td>1518</td>\n",
       "      <td>21.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>42.6</td>\n",
       "      <td>958</td>\n",
       "      <td>78.9</td>\n",
       "      <td>17.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11327</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>36005038800</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2471</td>\n",
       "      <td>953</td>\n",
       "      <td>1518</td>\n",
       "      <td>21.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>42.6</td>\n",
       "      <td>958</td>\n",
       "      <td>78.9</td>\n",
       "      <td>17.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11328</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36005039800</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2769</td>\n",
       "      <td>1338</td>\n",
       "      <td>1431</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>41.3</td>\n",
       "      <td>1404</td>\n",
       "      <td>76.2</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11329</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36005039800</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2769</td>\n",
       "      <td>1338</td>\n",
       "      <td>1431</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>41.3</td>\n",
       "      <td>1404</td>\n",
       "      <td>76.2</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11330</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36005030100</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>1308</td>\n",
       "      <td>524</td>\n",
       "      <td>784</td>\n",
       "      <td>32.6</td>\n",
       "      <td>59.6</td>\n",
       "      <td>...</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>455</td>\n",
       "      <td>80.9</td>\n",
       "      <td>10.3</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11331</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36005030100</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>1308</td>\n",
       "      <td>524</td>\n",
       "      <td>784</td>\n",
       "      <td>32.6</td>\n",
       "      <td>59.6</td>\n",
       "      <td>...</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>455</td>\n",
       "      <td>80.9</td>\n",
       "      <td>10.3</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11332</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36005030100</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>1308</td>\n",
       "      <td>524</td>\n",
       "      <td>784</td>\n",
       "      <td>32.6</td>\n",
       "      <td>59.6</td>\n",
       "      <td>...</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>455</td>\n",
       "      <td>80.9</td>\n",
       "      <td>10.3</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11333</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>36005042400</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2686</td>\n",
       "      <td>1387</td>\n",
       "      <td>1299</td>\n",
       "      <td>12.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>56.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>80.1</td>\n",
       "      <td>16.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11334</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>36005042400</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2686</td>\n",
       "      <td>1387</td>\n",
       "      <td>1299</td>\n",
       "      <td>12.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>56.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>80.1</td>\n",
       "      <td>16.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11335</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36005042400</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2686</td>\n",
       "      <td>1387</td>\n",
       "      <td>1299</td>\n",
       "      <td>12.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>56.4</td>\n",
       "      <td>1118</td>\n",
       "      <td>80.1</td>\n",
       "      <td>16.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11336</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36005044800</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2181</td>\n",
       "      <td>1092</td>\n",
       "      <td>1089</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1233</td>\n",
       "      <td>77.9</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11337</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36005044800</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2181</td>\n",
       "      <td>1092</td>\n",
       "      <td>1089</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1233</td>\n",
       "      <td>77.9</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11338</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36005044800</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>2181</td>\n",
       "      <td>1092</td>\n",
       "      <td>1089</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1233</td>\n",
       "      <td>77.9</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11339</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>36005043000</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>3321</td>\n",
       "      <td>1380</td>\n",
       "      <td>1941</td>\n",
       "      <td>16.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.2</td>\n",
       "      <td>1466</td>\n",
       "      <td>76.2</td>\n",
       "      <td>22.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11340</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>36005043000</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>3321</td>\n",
       "      <td>1380</td>\n",
       "      <td>1941</td>\n",
       "      <td>16.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.2</td>\n",
       "      <td>1466</td>\n",
       "      <td>76.2</td>\n",
       "      <td>22.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11341 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Race  Num_Arrests  CensusTract    County        Borough  TotalPop  \\\n",
       "0         0            1  36085024402  Richmond  Staten Island      4241   \n",
       "1         0            4  36085024402  Richmond  Staten Island      4241   \n",
       "2         0            1  36085024402  Richmond  Staten Island      4241   \n",
       "3         0            1  36085024402  Richmond  Staten Island      4241   \n",
       "4         0            1  36085024402  Richmond  Staten Island      4241   \n",
       "...     ...          ...          ...       ...            ...       ...   \n",
       "11336     1            2  36005044800     Bronx          Bronx      2181   \n",
       "11337     1            2  36005044800     Bronx          Bronx      2181   \n",
       "11338     1            2  36005044800     Bronx          Bronx      2181   \n",
       "11339     1            5  36005043000     Bronx          Bronx      3321   \n",
       "11340     1            6  36005043000     Bronx          Bronx      3321   \n",
       "\n",
       "        Men  Women  Hispanic  White  ...  Walk  OtherTransp  WorkAtHome  \\\n",
       "0      2023   2218       3.7   84.5  ...   1.1          0.6         4.0   \n",
       "1      2023   2218       3.7   84.5  ...   1.1          0.6         4.0   \n",
       "2      2023   2218       3.7   84.5  ...   1.1          0.6         4.0   \n",
       "3      2023   2218       3.7   84.5  ...   1.1          0.6         4.0   \n",
       "4      2023   2218       3.7   84.5  ...   1.1          0.6         4.0   \n",
       "...     ...    ...       ...    ...  ...   ...          ...         ...   \n",
       "11336  1092   1089       9.9    1.6  ...   2.8          0.0         0.0   \n",
       "11337  1092   1089       9.9    1.6  ...   2.8          0.0         0.0   \n",
       "11338  1092   1089       9.9    1.6  ...   2.8          0.0         0.0   \n",
       "11339  1380   1941      16.2    2.2  ...   5.5          0.0         0.0   \n",
       "11340  1380   1941      16.2    2.2  ...   5.5          0.0         0.0   \n",
       "\n",
       "       MeanCommute  Employed  PrivateWork  PublicWork  SelfEmployed  \\\n",
       "0             44.3      2046         75.2        21.2           3.6   \n",
       "1             44.3      2046         75.2        21.2           3.6   \n",
       "2             44.3      2046         75.2        21.2           3.6   \n",
       "3             44.3      2046         75.2        21.2           3.6   \n",
       "4             44.3      2046         75.2        21.2           3.6   \n",
       "...            ...       ...          ...         ...           ...   \n",
       "11336         49.0      1233         77.9        19.6           2.5   \n",
       "11337         49.0      1233         77.9        19.6           2.5   \n",
       "11338         49.0      1233         77.9        19.6           2.5   \n",
       "11339         38.2      1466         76.2        22.4           1.4   \n",
       "11340         38.2      1466         76.2        22.4           1.4   \n",
       "\n",
       "       FamilyWork  Unemployment  \n",
       "0             0.0           8.3  \n",
       "1             0.0           8.3  \n",
       "2             0.0           8.3  \n",
       "3             0.0           8.3  \n",
       "4             0.0           8.3  \n",
       "...           ...           ...  \n",
       "11336         0.0          10.7  \n",
       "11337         0.0          10.7  \n",
       "11338         0.0          10.7  \n",
       "11339         0.0           9.5  \n",
       "11340         0.0           9.5  \n",
       "\n",
       "[11341 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrest_data = pd.read_csv(\"arrests_w_census_loc.csv\")\n",
    "arrest_data.PERP_RACE[arrest_data.PERP_RACE.str.contains(\"WHITE\")]=0\n",
    "arrest_data.PERP_RACE[arrest_data.PERP_RACE != 0]=1\n",
    "arrest_data = arrest_data.groupby([\"PERP_RACE\",\"BlockLocation\"]).size().reset_index(name='counts')\n",
    "blockLocation = arrest_data[\"BlockLocation\"]\n",
    "blockLat = [float(re.findall(r'[-\\d\\.]+', bl)[0]) for bl in blockLocation]\n",
    "blockLon = [float(re.findall(r'[-\\d\\.]+', bl)[1]) for bl in blockLocation]\n",
    "arrest_data[\"blockLat\"]=blockLat\n",
    "arrest_data[\"blockLon\"]=blockLon\n",
    "arrest_data = arrest_data.drop(\"BlockLocation\", axis=1)\n",
    "arrest_data = arrest_data.rename(columns={\"counts\": \"Num_Arrests\", \"PERP_RACE\": \"Race\"})\n",
    "block_data = pd.read_csv(\"census_block_loc.csv\")\n",
    "block_data = pd.merge(left=arrest_data, right=block_data,\n",
    "                      left_on=[\"blockLat\",\"blockLon\"], right_on=[\"Latitude\",\"Longitude\"])\n",
    "census_data = pd.read_csv(\"nyc_census_tracts.csv\")\n",
    "tracts = block_data[\"BlockCode\"]\n",
    "tracts = [int(str(tract)[:-4]) for tract in tracts]\n",
    "block_data[\"tracts\"]=tracts\n",
    "block_data = block_data.drop(columns=[\"Latitude\",\"Longitude\",\"BlockCode\",\"County\",\"blockLat\",\"blockLon\"])\n",
    "data = pd.merge(left=block_data, right=census_data, left_on=\"tracts\", right_on=\"CensusTract\")\n",
    "data = data.drop(\"tracts\", axis=1)\n",
    "data = data.drop(\"State\", axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Outlier Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11341 Samples Before Outlier Removal\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASr0lEQVR4nO3df6zd9X3f8eerOCEbVDGU7Mo11kwVr5WzKcCugCj94yYsYOg0p1IWgapgJUzuH6AlU6TNdH/QNkOiUhPWSCmKO7yQKovL8mNYFBW5LldV/uCHaRnBEMYNkGKL4DY4pJdoUZ2998f5mJw4tu+9x9fn+p7P8yEdne/3/f18z/m8+Vqvc873fM8lVYUkqQ8/t9ITkCSNj6EvSR0x9CWpI4a+JHXE0JekjqxZ6QmcykUXXVQbN24cad833niD8847b3kndJay18lkr5NpHL0+8cQTf1dV7zjRtrM69Ddu3Mj+/ftH2nd2dpaZmZnlndBZyl4nk71OpnH0muQ7J9vm6R1J6siCoZ/kbUkeS/K/kxxI8jutfkmSR5PMJfmTJG9t9XPb+lzbvnHosW5r9eeSXHummpIkndhi3un/CHh/Vb0buBTYkuQq4PeAu6rqncAR4OY2/mbgSKvf1caRZDNwA/AuYAvwh0nOWc5mJEmntmDo18B8W31LuxXwfuArrX4v8MG2vLWt07ZfnSStvruqflRVLwJzwBXL0oUkaVEW9UVue0f+BPBO4HPAt4HvV9XRNuQgsL4trwdeBqiqo0leB36h1R8ZetjhfYafazuwHWBqaorZ2dmlddTMz8+PvO9qY6+TyV4n00r3uqjQr6ofA5cmWQt8HfiVMzWhqtoJ7ASYnp6uUb/l9mqAyWSvk8lex2dJV+9U1feBh4H3AGuTHHvRuBg41JYPARsA2va3A98brp9gH0nSGCzm6p13tHf4JPlHwAeAZxmE/4fasG3A/W15T1unbf+LGvz95j3ADe3qnkuATcBjy9WIJGlhizm9sw64t53X/zngvqp6IMkzwO4k/wX4a+CeNv4e4I+TzAGvMbhih6o6kOQ+4BngKHBLO20kSRqTBUO/qp4CLjtB/QVOcPVNVf1f4N+e5LHuAO5Y+jQlScvBX+RKUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyIKhn2RDkoeTPJPkQJKPt/pvJzmU5Ml2u35on9uSzCV5Lsm1Q/UtrTaXZMeZaUmSdDJrFjHmKPDJqvqrJD8PPJFkb9t2V1X9/vDgJJuBG4B3Ab8I/HmSf9Y2fw74AHAQeDzJnqp6ZjkakSQtbMHQr6pXgFfa8t8neRZYf4pdtgK7q+pHwItJ5oAr2ra5qnoBIMnuNtbQl6QxWcw7/Tcl2QhcBjwKvBe4NclNwH4GnwaOMHhBeGRot4P85EXi5ePqV57gObYD2wGmpqaYnZ1dyhTfND8/P/K+q429TiZ7nUwr3euiQz/J+cBXgU9U1Q+S3A18Cqh2/2ngY6c7oaraCewEmJ6erpmZmZEeZ3Z2llH3XW3sdTLZ62Ra6V4XFfpJ3sIg8L9UVV8DqKpXh7b/EfBAWz0EbBja/eJW4xR1SdIYLObqnQD3AM9W1WeG6uuGhv068HRb3gPckOTcJJcAm4DHgMeBTUkuSfJWBl/27lmeNiRJi7GYd/rvBT4CfDPJk632W8CNSS5lcHrnJeA3AarqQJL7GHxBexS4pap+DJDkVuAh4BxgV1UdWMZeJEkLWMzVO98AcoJND55inzuAO05Qf/BU+0mSzix/kStJHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjC4Z+kg1JHk7yTJIDST7e6hcm2Zvk+XZ/QasnyWeTzCV5KsnlQ4+1rY1/Psm2M9eWJOlEFvNO/yjwyaraDFwF3JJkM7AD2FdVm4B9bR3gOmBTu20H7obBiwRwO3AlcAVw+7EXCknSeCwY+lX1SlX9VVv+e+BZYD2wFbi3DbsX+GBb3gp8sQYeAdYmWQdcC+ytqteq6giwF9iyrN1Ikk5pzVIGJ9kIXAY8CkxV1Stt03eBqba8Hnh5aLeDrXay+vHPsZ3BJwSmpqaYnZ1dyhTfND8/P/K+q429TiZ7nUwr3euiQz/J+cBXgU9U1Q+SvLmtqipJLceEqmonsBNgenq6ZmZmRnqc2dlZRt13tbHXyWSvk2mle13U1TtJ3sIg8L9UVV9r5VfbaRva/eFWPwRsGNr94lY7WV2SNCaLuXonwD3As1X1maFNe4BjV+BsA+4fqt/UruK5Cni9nQZ6CLgmyQXtC9xrWk2SNCaLOb3zXuAjwDeTPNlqvwXcCdyX5GbgO8CH27YHgeuBOeCHwEcBquq1JJ8CHm/jfreqXluWLiRJi7Jg6FfVN4CcZPPVJxhfwC0neaxdwK6lTFCStHz8Ra4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1JEFQz/JriSHkzw9VPvtJIeSPNlu1w9tuy3JXJLnklw7VN/SanNJdix/K5KkhSzmnf4XgC0nqN9VVZe224MASTYDNwDvavv8YZJzkpwDfA64DtgM3NjGSpLGaM1CA6rqL5NsXOTjbQV2V9WPgBeTzAFXtG1zVfUCQJLdbewzS56xJGlkC4b+Kdya5CZgP/DJqjoCrAceGRpzsNUAXj6ufuWJHjTJdmA7wNTUFLOzsyNNbn5+fuR9Vxt7nUz2OplWutdRQ/9u4FNAtftPAx9bjglV1U5gJ8D09HTNzMyM9Dizs7OMuu9qY6+TyV4n00r3OlLoV9Wrx5aT/BHwQFs9BGwYGnpxq3GKuiRpTEa6ZDPJuqHVXweOXdmzB7ghyblJLgE2AY8BjwObklyS5K0MvuzdM/q0JUmjWPCdfpIvAzPARUkOArcDM0kuZXB65yXgNwGq6kCS+xh8QXsUuKWqftwe51bgIeAcYFdVHVj2biRJp7SYq3duPEH5nlOMvwO44wT1B4EHlzQ7SdKy8he5ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOrJg6CfZleRwkqeHahcm2Zvk+XZ/QasnyWeTzCV5KsnlQ/tsa+OfT7LtzLQjSTqVxbzT/wKw5bjaDmBfVW0C9rV1gOuATe22HbgbBi8SwO3AlcAVwO3HXigkSeOzYOhX1V8Crx1X3grc25bvBT44VP9iDTwCrE2yDrgW2FtVr1XVEWAvP/tCIkk6w9aMuN9UVb3Slr8LTLXl9cDLQ+MOttrJ6j8jyXYGnxKYmppidnZ2pAnOz8+PvO9qY6+TyV4n00r3Omrov6mqKkktx2Ta4+0EdgJMT0/XzMzMSI8zOzvLqPuuNvY6mex1Mq10r6NevfNqO21Duz/c6oeADUPjLm61k9UlSWM0aujvAY5dgbMNuH+oflO7iucq4PV2Gugh4JokF7QvcK9pNUnSGC14eifJl4EZ4KIkBxlchXMncF+Sm4HvAB9uwx8ErgfmgB8CHwWoqteSfAp4vI373ao6/sthSdIZtmDoV9WNJ9l09QnGFnDLSR5nF7BrSbOTJC0rf5ErSR0x9CWpI4a+JHVkokN/444/XekpSNJZZaJDX5L00wx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI6cVugneSnJN5M8mWR/q12YZG+S59v9Ba2eJJ9NMpfkqSSXL0cDkqTFW453+u+rqkurarqt7wD2VdUmYF9bB7gO2NRu24G7l+G5JUlLcCZO72wF7m3L9wIfHKp/sQYeAdYmWXcGnl+SdBKpqtF3Tl4EjgAFfL6qdib5flWtbdsDHKmqtUkeAO6sqm+0bfuA/1RV+497zO0MPgkwNTX1L3fv3j3S3Obn53nx9R/zL9a/fdT2Vo35+XnOP//8lZ7GWNjrZLLX5fW+973viaGzLz9lzWk+9q9W1aEk/wTYm+RbwxurqpIs6VWlqnYCOwGmp6drZmZmpInNzs7y6W+8wUu/Mdr+q8ns7Cyj/ndabex1Mtnr+JzW6Z2qOtTuDwNfB64AXj122qbdH27DDwEbhna/uNUkSWMycugnOS/Jzx9bBq4Bngb2ANvasG3A/W15D3BTu4rnKuD1qnpl5JlLkpbsdE7vTAFfH5y2Zw3wP6rqz5I8DtyX5GbgO8CH2/gHgeuBOeCHwEdP47klSSMYOfSr6gXg3Seofw+4+gT1Am4Z9fkkSafPX+RKUkcmPvQ37vjTlZ6CJJ01Jj70JUk/YehLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjrSTej7f9CSpI5CX5Jk6EtSVwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOjD30k2xJ8lySuSQ7xv38ktSzsYZ+knOAzwHXAZuBG5NsHuccJKlna8b8fFcAc1X1AkCS3cBW4JkxzwMY/Er3pTt/7aS/1n3pzl8b84wk6cxKVY3vyZIPAVuq6t+19Y8AV1bVrUNjtgPb2+ovA8+N+HQXAX93GtNdTex1MtnrZBpHr/+0qt5xog3jfqe/oKraCew83cdJsr+qppdhSmc9e51M9jqZVrrXcX+RewjYMLR+catJksZg3KH/OLApySVJ3grcAOwZ8xwkqVtjPb1TVUeT3Ao8BJwD7KqqA2fo6U77FNEqYq+TyV4n04r2OtYvciVJK8tf5EpSRwx9SerIRIb+pP2phyQbkjyc5JkkB5J8vNUvTLI3yfPt/oJWT5LPtv6fSnL5ynawNEnOSfLXSR5o65ckebT18yftIgCSnNvW59r2jSs576VKsjbJV5J8K8mzSd4zwcf0P7R/u08n+XKSt03ScU2yK8nhJE8P1ZZ8LJNsa+OfT7LtTMx14kJ/Qv/Uw1Hgk1W1GbgKuKX1tAPYV1WbgH1tHQa9b2q37cDd45/yafk48OzQ+u8Bd1XVO4EjwM2tfjNwpNXvauNWkz8A/qyqfgV4N4OeJ+6YJlkP/Htguqr+OYOLOG5gso7rF4Atx9WWdCyTXAjcDlzJ4K8X3H7shWJZVdVE3YD3AA8Nrd8G3LbS81rmHu8HPsDg18rrWm0d8Fxb/jxw49D4N8ed7TcGv93YB7wfeAAIg18vrjn++DK4Cuw9bXlNG5eV7mGRfb4dePH4+U7oMV0PvAxc2I7TA8C1k3ZcgY3A06MeS+BG4PND9Z8at1y3iXunz0/+gR1zsNUmQvuoexnwKDBVVa+0Td8Fptryav5v8F+B/wj8v7b+C8D3q+poWx/u5c0+2/bX2/jV4BLgb4H/3k5l/bck5zGBx7SqDgG/D/wN8AqD4/QEk3lchy31WI7lGE9i6E+sJOcDXwU+UVU/GN5Wg7cGq/r62yT/GjhcVU+s9FzGYA1wOXB3VV0GvMFPPv4Dk3FMAdopiq0MXuh+ETiPnz0VMtHOpmM5iaE/kX/qIclbGAT+l6rqa638apJ1bfs64HCrr9b/Bu8F/k2Sl4DdDE7x/AGwNsmxHxIO9/Jmn23724HvjXPCp+EgcLCqHm3rX2HwIjBpxxTgXwEvVtXfVtU/AF9jcKwn8bgOW+qxHMsxnsTQn7g/9ZAkwD3As1X1maFNe4Bj3/BvY3Cu/1j9pnaVwFXA60MfM89aVXVbVV1cVRsZHLe/qKrfAB4GPtSGHd/nsf4/1MafFe+mFlJV3wVeTvLLrXQ1gz8xPlHHtPkb4Kok/7j9Wz7W68Qd1+Ms9Vg+BFyT5IL26eiaVlteK/3lxxn6QuV64P8A3wb+80rPZxn6+VUGHw2fAp5st+sZnOfcBzwP/DlwYRsfBlcwfRv4JoOrJla8jyX2PAM80JZ/CXgMmAP+J3Buq7+trc+17b+00vNeYo+XAvvbcf1fwAWTekyB3wG+BTwN/DFw7iQdV+DLDL6v+AcGn+JuHuVYAh9rfc8BHz0Tc/XPMEhSRybx9I4k6SQMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktSR/w94aXtHVH35HQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11021 Samples After Outlier Removal\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.1783793316286% of Samples were Kept\n"
     ]
    }
   ],
   "source": [
    "data[\"Num_Arrests\"].hist(bins='auto')\n",
    "data_size=data.shape[0]\n",
    "print(\"There are\",data_size,\"Samples Before Outlier Removal\")\n",
    "plt.show()\n",
    "data = data[data[\"Num_Arrests\"]<50]\n",
    "new_size=data.shape[0]\n",
    "print(\"There are\",new_size,\"Samples After Outlier Removal\")\n",
    "data[\"Num_Arrests\"].hist(bins='auto')\n",
    "plt.show()\n",
    "print(100*new_size/data_size,\"% of Samples were Kept\",sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Aggregates by County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_all(data):\n",
    "    new_data = {}\n",
    "    for colname in ['Num_Arrests','TotalPop','Men','Women','Citizen','Income','IncomeErr', 'Employed']:\n",
    "        new_data[colname]=data[colname].sum()\n",
    "    for colname in ['Hispanic', 'White', 'Black', 'Native', 'Asian','IncomePerCap','IncomePerCapErr','Poverty', 'ChildPoverty', 'Professional', 'Service', 'Office',\n",
    "            'Construction', 'Production', 'Drive', 'Carpool', 'Transit', 'Walk','OtherTransp', 'WorkAtHome',\n",
    "           'MeanCommute','PrivateWork','PublicWork','SelfEmployed','FamilyWork','Unemployment']:\n",
    "        ratio = data[colname]*data['TotalPop']\n",
    "        new_data[colname]=(ratio.sum()/100)/data.size\n",
    "    return pd.Series(new_data)\n",
    "\n",
    "agg_data = data.groupby(['County','Race']).apply(agg_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attatch Aggregate to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Race</th>\n",
       "      <th>Num_Arrests</th>\n",
       "      <th>CensusTract</th>\n",
       "      <th>County</th>\n",
       "      <th>Borough</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>...</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>County_Num_Arrests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.238152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.238152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.238152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.238152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36085024402</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>4241</td>\n",
       "      <td>2023</td>\n",
       "      <td>2218</td>\n",
       "      <td>3.7</td>\n",
       "      <td>84.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.3</td>\n",
       "      <td>2046</td>\n",
       "      <td>75.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.238152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Race  Num_Arrests  CensusTract    County        Borough  TotalPop   Men  \\\n",
       "0     0            1  36085024402  Richmond  Staten Island      4241  2023   \n",
       "1     0            4  36085024402  Richmond  Staten Island      4241  2023   \n",
       "2     0            1  36085024402  Richmond  Staten Island      4241  2023   \n",
       "3     0            1  36085024402  Richmond  Staten Island      4241  2023   \n",
       "4     0            1  36085024402  Richmond  Staten Island      4241  2023   \n",
       "\n",
       "   Women  Hispanic  White  ...  OtherTransp  WorkAtHome  MeanCommute  \\\n",
       "0   2218       3.7   84.5  ...          0.6         4.0         44.3   \n",
       "1   2218       3.7   84.5  ...          0.6         4.0         44.3   \n",
       "2   2218       3.7   84.5  ...          0.6         4.0         44.3   \n",
       "3   2218       3.7   84.5  ...          0.6         4.0         44.3   \n",
       "4   2218       3.7   84.5  ...          0.6         4.0         44.3   \n",
       "\n",
       "   Employed  PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \\\n",
       "0      2046         75.2        21.2           3.6         0.0           8.3   \n",
       "1      2046         75.2        21.2           3.6         0.0           8.3   \n",
       "2      2046         75.2        21.2           3.6         0.0           8.3   \n",
       "3      2046         75.2        21.2           3.6         0.0           8.3   \n",
       "4      2046         75.2        21.2           3.6         0.0           8.3   \n",
       "\n",
       "   County_Num_Arrests  \n",
       "0            2.238152  \n",
       "1            2.238152  \n",
       "2            2.238152  \n",
       "3            2.238152  \n",
       "4            2.238152  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agg_data[\"arrest_ratio\"]=agg_data[\"Num_Arrests\"]/agg_data[\"TotalPop\"]\n",
    "arrest_ratio = list(agg_data[\"arrest_ratio\"])\n",
    "county = list(agg_data.index.get_level_values(0))\n",
    "race = list(agg_data.index.get_level_values(1))\n",
    "agg_data = pd.DataFrame(arrest_ratio, columns=[\"arrest_ratio\"])\n",
    "agg_data[\"County\"]=county\n",
    "agg_data[\"Race\"]=race\n",
    "data = pd.merge(data, agg_data, on=[\"County\",\"Race\"])\n",
    "data[\"County_Num_Arrests\"]=data[\"arrest_ratio\"]*data[\"TotalPop\"]\n",
    "data = data.drop(\"arrest_ratio\", axis=1)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Z Scaling to Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['TotalPop','Men','Women','Hispanic','White','Black','Native','Asian','Citizen','Income','IncomeErr','IncomePerCap','IncomePerCapErr','Poverty','ChildPoverty','Professional','Service','Office','Construction','Production','Drive','Carpool','Transit','Walk','OtherTransp','WorkAtHome','MeanCommute','Employed','PrivateWork','PublicWork','SelfEmployed','FamilyWork','Unemployment']\n",
    "X = data[num_cols].values\n",
    "data.drop(columns=num_cols)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "num_data = pd.DataFrame(X, columns=num_cols)\n",
    "data[num_cols]=num_data\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data by Race and Into Train/Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.shape[1]\n",
    "data_white = data[data[\"Race\"]==0].drop(\"Race\", axis=1)\n",
    "data_non_white=data[data[\"Race\"]==1].drop(\"Race\", axis=1)\n",
    "data_white_train = data_white.sample(frac=0.8,random_state=101)\n",
    "data_white_test  = data_white.drop(data_white_train.index)\n",
    "data_non_white_train = data_non_white.sample(frac=0.8,random_state=101)\n",
    "data_non_white_test  = data_non_white.drop(data_non_white_train.index)\n",
    "Xw = data_white_train.iloc[:,1:cols]\n",
    "yw = data_white_train.iloc[:,0]\n",
    "Xnw = data_non_white_train.iloc[:,1:cols]\n",
    "ynw = data_non_white_train.iloc[:,0]\n",
    "Xwt = data_white_test.iloc[:,1:cols]\n",
    "ywt = data_white_test.iloc[:,0]\n",
    "Xnwt = data_non_white_test.iloc[:,1:cols]\n",
    "ynwt = data_non_white_test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Categorical Features as Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['CensusTract','County','Borough']\n",
    "encoder = LabelEncoder()\n",
    "for var in cat_cols:\n",
    "    Xw[var] = encoder.fit_transform(Xw[var])\n",
    "    Xnw[var] = encoder.fit_transform(Xnw[var])\n",
    "    Xwt[var] = encoder.fit_transform(Xwt[var])\n",
    "    Xnwt[var] = encoder.fit_transform(Xnwt[var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Test of Nonlinear Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8b26ae596904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrbfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbfw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neg_mean_squared_error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The average MSE for White RBF is\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrbfnw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 232\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rbfw = SVR(gamma='scale',kernel='rbf')\n",
    "cv = cross_val_score(rbfw, Xw, yw, cv=50, scoring=\"neg_mean_squared_error\")\n",
    "print(\"The average MSE for White RBF is\",-sum(cv)/len(cv))\n",
    "\n",
    "rbfnw = SVR(gamma='scale',kernel='rbf')\n",
    "cv = cross_val_score(rbfnw, Xnw, ynw, cv=50, scoring=\"neg_mean_squared_error\")\n",
    "print(\"The average MSE for Non-White RBF is\",-sum(cv)/len(cv))\n",
    "\n",
    "polyw = SVR(gamma='scale',kernel='poly')\n",
    "cv = cross_val_score(polyw, Xw, yw, cv=50, scoring=\"neg_mean_squared_error\")\n",
    "print(\"The average MSE for White Polynomial is\",-sum(cv)/len(cv))\n",
    "\n",
    "polynw = SVR(gamma='scale',kernel='poly')\n",
    "cv = cross_val_score(polynw, Xnw, ynw, cv=50, scoring=\"neg_mean_squared_error\")\n",
    "print(\"The average MSE for Non-White Polynomial is\",-sum(cv)/len(cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search to Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal white C 500\n",
      "optimal white epsilon 1.0910448648414965\n",
      "optimal white kernel rbf\n",
      "optimal non-white C 1000\n",
      "optimal non-white epsilon 2.6259597935782515\n",
      "optimal non-white kernel rbf\n"
     ]
    }
   ],
   "source": [
    "params = {\"C\":[0.1,0.5,1,5,10,50,100,500,1000],\"epsilon\":uniform(1,3),\"kernel\":[\"poly\",\"rbf\"]}\n",
    "\n",
    "rscv = RandomizedSearchCV(SVR(gamma=\"scale\"),params,cv=50,iid=False)\n",
    "rscv.fit(Xw,yw)\n",
    "print(\"optimal white C\", rscv.best_estimator_.get_params()[\"C\"])\n",
    "print(\"optimal white epsilon\", rscv.best_estimator_.get_params()[\"epsilon\"])\n",
    "print(\"optimal white kernel\", rscv.best_estimator_.get_params()[\"kernel\"])\n",
    "\n",
    "rscv2 = RandomizedSearchCV(SVR(gamma=\"scale\"),params,cv=50,iid=False)\n",
    "rscv2.fit(Xnw,ynw)\n",
    "print(\"optimal non-white C\", rscv2.best_estimator_.get_params()[\"C\"])\n",
    "print(\"optimal non-white epsilon\", rscv2.best_estimator_.get_params()[\"epsilon\"])\n",
    "print(\"optimal non-white kernel\", rscv2.best_estimator_.get_params()[\"kernel\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE for White:  22.599631771725846\n",
      "Test MAE for White:  2.922924840204984\n",
      "Test MSE for Non-White:  31.391169452782947\n",
      "Test MAE for Non-White:  3.8686288382759195\n"
     ]
    }
   ],
   "source": [
    "Final_model = SVR(kernel='rbf', C=750, gamma=\"scale\", epsilon=1.5)\n",
    "Final_model.fit(Xw, yw)\n",
    "test_predicted = Final_model.predict(Xwt)\n",
    "mse = mean_squared_error(ywt, test_predicted)\n",
    "print(\"Test MSE for White: \",mse)\n",
    "mae = mean_absolute_error(ywt, test_predicted)\n",
    "print(\"Test MAE for White: \",mae)\n",
    "Final_model.fit(Xnw, ynw)\n",
    "test_predicted_2 = Final_model.predict(Xnwt)\n",
    "mse = mean_squared_error(ynwt, test_predicted_2)\n",
    "print(\"Test MSE for Non-White: \",mse)\n",
    "mae = mean_absolute_error(ynwt, test_predicted_2)\n",
    "print(\"Test MAE for Non-White: \",mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TestAccount\\Miniconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average MSE for White is 21.09364132212654\n",
      "The average MSE for Non-White is 32.210134068711675\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(Xw, yw)\n",
    "cv = cross_val_score(model, Xw, yw, cv=50, scoring=\"neg_mean_squared_error\")\n",
    "print(\"The average MSE for White is\",-sum(cv)/len(cv))\n",
    "model.fit(Xnw, ynw)\n",
    "cv = cross_val_score(model, Xnw, ynw, cv=50, scoring=\"neg_mean_squared_error\")\n",
    "print(\"The average MSE for Non-White is\",-sum(cv)/len(cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Random Forrest Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal min_samples_split 25\n",
      "optimal max_features 20\n",
      "optimal min_samples_split 20\n",
      "optimal max_features 5\n"
     ]
    }
   ],
   "source": [
    "params = {\"min_samples_split\":[2,5,10,15,20,25,30,35], \"max_features\":[5,10,15,20,25,30]}\n",
    "\n",
    "rscv = RandomizedSearchCV(RandomForestRegressor(n_estimators=500),params,cv=50,iid=False)\n",
    "rscv.fit(Xw,yw)\n",
    "print(\"optimal white min_samples_split\", rscv.best_estimator_.get_params()[\"min_samples_split\"])\n",
    "print(\"optimal white max_features\", rscv.best_estimator_.get_params()[\"max_features\"])\n",
    "\n",
    "rscv2 = RandomizedSearchCV(RandomForestRegressor(n_estimators=500),params,cv=50,iid=False)\n",
    "rscv2.fit(Xnw,ynw)\n",
    "print(\"optimal non-white min_samples_split\", rscv2.best_estimator_.get_params()[\"min_samples_split\"])\n",
    "print(\"optimal non-white max_features\", rscv2.best_estimator_.get_params()[\"max_features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Data From Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE for White:  30.272413103684947\n",
      "Test MAE for White:  3.374957875879353\n",
      "Test MSE for Non-White:  53.91413794852636\n",
      "Test MAE for Non-White:  4.826558794942388\n"
     ]
    }
   ],
   "source": [
    "Final_model = RandomForestRegressor(n_estimators=500, min_samples_split=25, max_features=10)\n",
    "Final_model.fit(Xw, yw)\n",
    "test_predicted = Final_model.predict(Xwt)\n",
    "mse = mean_squared_error(ywt, test_predicted)\n",
    "print(\"Test MSE for White: \",mse)\n",
    "mae = mean_absolute_error(ywt, test_predicted)\n",
    "print(\"Test MAE for White: \",mae)\n",
    "Final_model.fit(Xnw, ynw)\n",
    "test_predicted_2 = Final_model.predict(Xnwt)\n",
    "mse = mean_squared_error(ynwt, test_predicted_2)\n",
    "print(\"Test MSE for Non-White: \",mse)\n",
    "mae = mean_absolute_error(ynwt, test_predicted_2)\n",
    "print(\"Test MAE for Non-White: \",mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bias Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_model.fit(Xw, yw)\n",
    "Xw[\"predicted_w\"] = Final_model.predict(Xw)\n",
    "Final_model.fit(Xnw, ynw)\n",
    "Xnw[\"predicted_nw\"] = Final_model.predict(Xnw)\n",
    "Xnw = Xnw[[\"CensusTract\", \"predicted_nw\",\"County_Num_Arrests\"]]\n",
    "Xnw = Xnw.rename(columns={\"County_Num_Arrests\":\"County_Num_Arrests_nw\"})\n",
    "X = pd.merge(left=Xw, right=Xnw,left_on=\"CensusTract\", right_on=\"CensusTract\")\n",
    "X[\"Non-White\"]=1-X[\"White\"]\n",
    "X[\"y\"] = np.log(abs(X[\"predicted_w\"]/X[\"White\"]-X[\"predicted_nw\"]/X[\"Non-White\"]))\n",
    "X = X.replace(np.inf, np.nan)\n",
    "X = X.dropna()\n",
    "X_train = X.sample(frac=0.8,random_state=101)\n",
    "Xt=X.drop(X_train.index)\n",
    "X=X_train\n",
    "y = X[\"y\"]\n",
    "yt=Xt[\"y\"]\n",
    "X = X.drop(columns=['Hispanic','Black','Native','Asian',\"y\"])\n",
    "Xt = Xt.drop(columns=['Hispanic','Black','Native','Asian',\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bias Metric by County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1932    3.143018\n",
      "6552    3.699560\n",
      "9210    5.267216\n",
      "6230    2.245521\n",
      "1671    2.151401\n",
      "          ...   \n",
      "2178    3.900730\n",
      "4803    2.559835\n",
      "3603    2.258015\n",
      "7349    1.498583\n",
      "737     1.000193\n",
      "Name: y, Length: 8485, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "yc = np.log(abs(X[\"County_Num_Arrests\"]/X[\"White\"]-X[\"County_Num_Arrests_nw\"]/X[\"Non-White\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['yb']=y\n",
    "X['yc']=yc\n",
    "Xtrain = X.sample(frac=0.9,random_state=101)\n",
    "Xvalid = X.drop(Xtrain.index)\n",
    "ybtrain = Xtrain['yb']\n",
    "yctrain = Xtrain['yc']\n",
    "ybvalid = Xvalid['yb']\n",
    "ycvalid = Xvalid['yc']\n",
    "X = X.drop(columns=['yb','yc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Plot of MSE vs Smoothing Factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7636/7636 [==============================] - 0s 59us/step - loss: 1.1694\n",
      "Epoch 2/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.2132\n",
      "Epoch 3/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.1402\n",
      "Epoch 4/1000\n",
      "7636/7636 [==============================] - 0s 40us/step - loss: 0.0830\n",
      "Epoch 5/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.2609\n",
      "Epoch 6/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0122\n",
      "Epoch 7/1000\n",
      "7636/7636 [==============================] - 0s 40us/step - loss: 0.7574\n",
      "Epoch 8/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0076\n",
      "Epoch 9/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0057\n",
      "Epoch 10/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0832\n",
      "Epoch 11/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0476\n",
      "Epoch 12/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.1302\n",
      "Epoch 13/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0110\n",
      "Epoch 14/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.1800\n",
      "Epoch 15/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.1031\n",
      "Epoch 16/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.2470\n",
      "Epoch 17/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0044\n",
      "Epoch 18/1000\n",
      "7636/7636 [==============================] - 0s 52us/step - loss: 0.1021\n",
      "Epoch 19/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0096\n",
      "Epoch 20/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.1245\n",
      "Epoch 21/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0900\n",
      "Epoch 22/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.1274\n",
      "Epoch 23/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0123\n",
      "Epoch 24/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0788\n",
      "Epoch 25/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.5437\n",
      "Epoch 26/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0029\n",
      "Epoch 27/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 5.2468e-04\n",
      "Epoch 28/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.0258\n",
      "Epoch 29/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.6957\n",
      "Epoch 30/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 5.3557e-04\n",
      "Epoch 31/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 2.8765e-04\n",
      "Epoch 32/1000\n",
      "7636/7636 [==============================] - 0s 52us/step - loss: 0.0025\n",
      "Epoch 33/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.1634\n",
      "Epoch 34/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0532\n",
      "Epoch 35/1000\n",
      "7636/7636 [==============================] - 0s 61us/step - loss: 0.1275\n",
      "Epoch 36/1000\n",
      "7636/7636 [==============================] - 0s 64us/step - loss: 0.1061\n",
      "Epoch 37/1000\n",
      "7636/7636 [==============================] - 0s 56us/step - loss: 0.0048\n",
      "Epoch 38/1000\n",
      "7636/7636 [==============================] - 0s 55us/step - loss: 0.2333\n",
      "Epoch 39/1000\n",
      "7636/7636 [==============================] - 0s 51us/step - loss: 6.1696e-04\n",
      "Epoch 40/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.8244\n",
      "Epoch 41/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0013\n",
      "Epoch 42/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 6.4790e-04\n",
      "Epoch 43/1000\n",
      "7636/7636 [==============================] - 0s 48us/step - loss: 0.0051\n",
      "Epoch 44/1000\n",
      "7636/7636 [==============================] - 0s 59us/step - loss: 0.0791\n",
      "Epoch 45/1000\n",
      "7636/7636 [==============================] - 0s 55us/step - loss: 0.0204\n",
      "Epoch 46/1000\n",
      "7636/7636 [==============================] - 0s 54us/step - loss: 0.2051\n",
      "Epoch 47/1000\n",
      "7636/7636 [==============================] - 0s 54us/step - loss: 0.1834\n",
      "Epoch 48/1000\n",
      "7636/7636 [==============================] - 0s 52us/step - loss: 0.0034\n",
      "Epoch 49/1000\n",
      "7636/7636 [==============================] - 0s 53us/step - loss: 0.0314\n",
      "Epoch 50/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.3315\n",
      "Epoch 51/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.0038\n",
      "Epoch 52/1000\n",
      "7636/7636 [==============================] - 0s 53us/step - loss: 0.0014\n",
      "Epoch 53/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0566\n",
      "Epoch 54/1000\n",
      "7636/7636 [==============================] - 0s 40us/step - loss: 0.0546\n",
      "Epoch 55/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.1691\n",
      "Epoch 56/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0066\n",
      "Epoch 57/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0607\n",
      "Epoch 58/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0648\n",
      "Epoch 59/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.1957\n",
      "Epoch 60/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0089\n",
      "Epoch 61/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.1896\n",
      "Epoch 62/1000\n",
      "7636/7636 [==============================] - 0s 52us/step - loss: 0.0019\n",
      "Epoch 63/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0916\n",
      "Epoch 64/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.1580\n",
      "Epoch 65/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0014\n",
      "Epoch 66/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.2106\n",
      "Epoch 67/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0838\n",
      "Epoch 68/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 3.0359e-04\n",
      "Epoch 69/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0278\n",
      "Epoch 70/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0363\n",
      "Epoch 71/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.1268\n",
      "Epoch 72/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0608\n",
      "Epoch 73/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.2728\n",
      "Epoch 74/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 6.6860e-04\n",
      "Epoch 75/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 2.9913e-04\n",
      "Epoch 76/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0472\n",
      "Epoch 77/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.2299\n",
      "Epoch 78/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0012\n",
      "Epoch 79/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0456\n",
      "Epoch 80/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0709\n",
      "Epoch 81/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0289\n",
      "Epoch 82/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.1004\n",
      "Epoch 83/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.0493\n",
      "Epoch 84/1000\n",
      "7636/7636 [==============================] - 0s 47us/step - loss: 0.0875\n",
      "Epoch 85/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0154\n",
      "Epoch 86/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.1066\n",
      "Epoch 87/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0113\n",
      "Epoch 88/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.1094\n",
      "Epoch 89/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0029\n",
      "Epoch 90/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.1432\n",
      "Epoch 91/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.3297\n",
      "Epoch 92/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0038\n",
      "Epoch 93/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 2.2438e-04\n",
      "Epoch 94/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7636/7636 [==============================] - 0s 44us/step - loss: 3.1019e-04\n",
      "Epoch 95/1000\n",
      "7636/7636 [==============================] - 0s 41us/step - loss: 0.1072\n",
      "Epoch 96/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0274\n",
      "Epoch 97/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.4323\n",
      "Epoch 98/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 5.6023e-04\n",
      "Epoch 99/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 2.0585e-04\n",
      "Epoch 100/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0047\n",
      "Epoch 101/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0377\n",
      "Epoch 102/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0583\n",
      "Epoch 103/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0687\n",
      "Epoch 104/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0090\n",
      "Epoch 105/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.1356\n",
      "Epoch 106/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0182\n",
      "Epoch 107/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0890\n",
      "Epoch 108/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.1135\n",
      "Epoch 109/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0015\n",
      "Epoch 110/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.3428\n",
      "Epoch 111/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 8.9024e-04\n",
      "Epoch 112/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 2.4142e-04\n",
      "Epoch 113/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.1548\n",
      "Epoch 114/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 5.2057e-04\n",
      "Epoch 115/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0054\n",
      "Epoch 116/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.1232\n",
      "Epoch 117/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0168\n",
      "Epoch 118/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0803\n",
      "Epoch 119/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0489\n",
      "Epoch 120/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0488\n",
      "Epoch 121/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.2334\n",
      "Epoch 122/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0023\n",
      "Epoch 123/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 4.0556e-04\n",
      "Epoch 124/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0049\n",
      "Epoch 125/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.1644\n",
      "Epoch 126/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 8.2326e-04\n",
      "Epoch 127/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0634\n",
      "Epoch 128/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.0807\n",
      "Epoch 129/1000\n",
      "7636/7636 [==============================] - 0s 47us/step - loss: 0.0032\n",
      "Epoch 130/1000\n",
      "7636/7636 [==============================] - 0s 47us/step - loss: 0.2879\n",
      "Epoch 131/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 6.3100e-04\n",
      "Epoch 132/1000\n",
      "7636/7636 [==============================] - 0s 47us/step - loss: 1.7991e-04\n",
      "Epoch 133/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.0541\n",
      "Epoch 134/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.0068\n",
      "Epoch 135/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.1126\n",
      "Epoch 136/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0043\n",
      "Epoch 137/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0315\n",
      "Epoch 138/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0476\n",
      "Epoch 139/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.1160\n",
      "Epoch 140/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0024\n",
      "Epoch 141/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0621\n",
      "Epoch 142/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.0478\n",
      "Epoch 143/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.0443\n",
      "Epoch 144/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.0165\n",
      "Epoch 145/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.0551\n",
      "Epoch 146/1000\n",
      "7636/7636 [==============================] - 0s 43us/step - loss: 0.0900\n",
      "Epoch 147/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0142\n",
      "Epoch 148/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.1759\n",
      "Epoch 149/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 3.4875e-04\n",
      "Epoch 150/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 4.0983e-04\n",
      "Epoch 151/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.1286\n",
      "Epoch 152/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0018\n",
      "Epoch 153/1000\n",
      "7636/7636 [==============================] - 0s 42us/step - loss: 0.0716\n",
      "Epoch 154/1000\n",
      "7636/7636 [==============================] - 0s 49us/step - loss: 0.0021\n",
      "Epoch 155/1000\n",
      "7636/7636 [==============================] - 0s 46us/step - loss: 0.0597\n",
      "Epoch 156/1000\n",
      "7636/7636 [==============================] - 0s 47us/step - loss: 0.0224\n",
      "Epoch 157/1000\n",
      "7636/7636 [==============================] - 0s 47us/step - loss: 0.0320\n",
      "Epoch 158/1000\n",
      "7636/7636 [==============================] - 0s 45us/step - loss: 0.1652\n",
      "Epoch 159/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 3.8381e-04\n",
      "Epoch 160/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 0.1366\n",
      "Epoch 161/1000\n",
      "7636/7636 [==============================] - 0s 44us/step - loss: 8.1253e-04\n",
      "Epoch 162/1000\n",
      "2192/7636 [=======>......................] - ETA: 0s - loss: 4.7605e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-bb0979d7ad9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mystrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mybtrain\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybtrain\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0myctrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mysvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mybvalid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybvalid\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mycvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mystrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mysvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmeansqerrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim = Xtrain.shape[1], activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer = 'adam')\n",
    "meansqerrors={}\n",
    "for i in range(21):\n",
    "    j = i/20\n",
    "    ystrain = ybtrain-j*(ybtrain-yctrain)\n",
    "    ysvalid = ybvalid-j*(ybvalid-ycvalid)\n",
    "    model.fit(Xtrain, ystrain, epochs = 1000, batch_size = 16)\n",
    "    score = model.evaluate(xvalid, ysvalid)\n",
    "    meansqerrors[j]=score\n",
    "    print(j,score)\n",
    "plt.plot(meansqerrors.keys(), meansqerrors.values())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model to Best Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7982/7982 [==============================] - 0s 59us/step - loss: 231.3992 - mean_squared_error: 231.3993 - mean_absolute_error: 4.8388\n",
      "Epoch 2/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 0.9436 - mean_squared_error: 0.9436 - mean_absolute_error: 0.6663\n",
      "Epoch 3/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 0.8749 - mean_squared_error: 0.8749 - mean_absolute_error: 0.6375\n",
      "Epoch 4/100\n",
      "7982/7982 [==============================] - 0s 35us/step - loss: 0.8574 - mean_squared_error: 0.8574 - mean_absolute_error: 0.6301\n",
      "Epoch 5/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.8510 - mean_squared_error: 0.8510 - mean_absolute_error: 0.6296\n",
      "Epoch 6/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 0.8261 - mean_squared_error: 0.8261 - mean_absolute_error: 0.6156\n",
      "Epoch 7/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.8404 - mean_squared_error: 0.8404 - mean_absolute_error: 0.6261\n",
      "Epoch 8/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.8081 - mean_squared_error: 0.8081 - mean_absolute_error: 0.6097\n",
      "Epoch 9/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.8654 - mean_squared_error: 0.8654 - mean_absolute_error: 0.6349\n",
      "Epoch 10/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.8633 - mean_squared_error: 0.8633 - mean_absolute_error: 0.6377\n",
      "Epoch 11/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.8626 - mean_squared_error: 0.8626 - mean_absolute_error: 0.6376\n",
      "Epoch 12/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.8507 - mean_squared_error: 0.8507 - mean_absolute_error: 0.6293\n",
      "Epoch 13/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 0.8638 - mean_squared_error: 0.8638 - mean_absolute_error: 0.6380\n",
      "Epoch 14/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.9965 - mean_squared_error: 0.9965 - mean_absolute_error: 0.7002\n",
      "Epoch 15/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 1.0324 - mean_squared_error: 1.0324 - mean_absolute_error: 0.7097\n",
      "Epoch 16/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 0.8843 - mean_squared_error: 0.8843 - mean_absolute_error: 0.6478\n",
      "Epoch 17/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 0.9075 - mean_squared_error: 0.9075 - mean_absolute_error: 0.6547\n",
      "Epoch 18/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.0130 - mean_squared_error: 1.0130 - mean_absolute_error: 0.7098\n",
      "Epoch 19/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.9579 - mean_squared_error: 0.9579 - mean_absolute_error: 0.6863\n",
      "Epoch 20/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 1.0416 - mean_squared_error: 1.0416 - mean_absolute_error: 0.7220\n",
      "Epoch 21/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 1.1473 - mean_squared_error: 1.1473 - mean_absolute_error: 0.7589\n",
      "Epoch 22/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 1.0247 - mean_squared_error: 1.0247 - mean_absolute_error: 0.7154\n",
      "Epoch 23/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 1.1377 - mean_squared_error: 1.1377 - mean_absolute_error: 0.7556\n",
      "Epoch 24/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 0.9442 - mean_squared_error: 0.9442 - mean_absolute_error: 0.6806\n",
      "Epoch 25/100\n",
      "7982/7982 [==============================] - 0s 36us/step - loss: 1.0554 - mean_squared_error: 1.0554 - mean_absolute_error: 0.7224\n",
      "Epoch 26/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.0641 - mean_squared_error: 1.0641 - mean_absolute_error: 0.7280\n",
      "Epoch 27/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.2025 - mean_squared_error: 1.2025 - mean_absolute_error: 0.7744\n",
      "Epoch 28/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.0711 - mean_squared_error: 1.0711 - mean_absolute_error: 0.7296\n",
      "Epoch 29/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 0.9879 - mean_squared_error: 0.9879 - mean_absolute_error: 0.6936\n",
      "Epoch 30/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.0717 - mean_squared_error: 1.0717 - mean_absolute_error: 0.7254\n",
      "Epoch 31/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.2722 - mean_squared_error: 1.2722 - mean_absolute_error: 0.7945\n",
      "Epoch 32/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.2041 - mean_squared_error: 1.2041 - mean_absolute_error: 0.7850\n",
      "Epoch 33/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.0820 - mean_squared_error: 1.0820 - mean_absolute_error: 0.7318\n",
      "Epoch 34/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 1.3842 - mean_squared_error: 1.3842 - mean_absolute_error: 0.8304\n",
      "Epoch 35/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 1.1737 - mean_squared_error: 1.1737 - mean_absolute_error: 0.7572\n",
      "Epoch 36/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 0.9433 - mean_squared_error: 0.9433 - mean_absolute_error: 0.6746\n",
      "Epoch 37/100\n",
      "7982/7982 [==============================] - 0s 28us/step - loss: 0.9683 - mean_squared_error: 0.9683 - mean_absolute_error: 0.6857\n",
      "Epoch 38/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.0105 - mean_squared_error: 1.0105 - mean_absolute_error: 0.7084\n",
      "Epoch 39/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9980 - mean_squared_error: 0.9980 - mean_absolute_error: 0.7022\n",
      "Epoch 40/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 1.0345 - mean_squared_error: 1.0345 - mean_absolute_error: 0.7128\n",
      "Epoch 41/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9928 - mean_squared_error: 0.9928 - mean_absolute_error: 0.7015\n",
      "Epoch 42/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.3518 - mean_squared_error: 1.3518 - mean_absolute_error: 0.8472\n",
      "Epoch 43/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.3269 - mean_squared_error: 1.3269 - mean_absolute_error: 0.8203\n",
      "Epoch 44/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 0.9730 - mean_squared_error: 0.9730 - mean_absolute_error: 0.6910\n",
      "Epoch 45/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 0.9560 - mean_squared_error: 0.9560 - mean_absolute_error: 0.6807\n",
      "Epoch 46/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.0799 - mean_squared_error: 1.0799 - mean_absolute_error: 0.7274\n",
      "Epoch 47/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9391 - mean_squared_error: 0.9391 - mean_absolute_error: 0.6768\n",
      "Epoch 48/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9910 - mean_squared_error: 0.9910 - mean_absolute_error: 0.6974\n",
      "Epoch 49/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 1.1210 - mean_squared_error: 1.1210 - mean_absolute_error: 0.7578\n",
      "Epoch 50/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.9096 - mean_squared_error: 0.9096 - mean_absolute_error: 0.6590\n",
      "Epoch 51/100\n",
      "7982/7982 [==============================] - 0s 28us/step - loss: 1.2604 - mean_squared_error: 1.2604 - mean_absolute_error: 0.8019\n",
      "Epoch 52/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 0.9528 - mean_squared_error: 0.9528 - mean_absolute_error: 0.6809\n",
      "Epoch 53/100\n",
      "7982/7982 [==============================] - 0s 28us/step - loss: 1.2008 - mean_squared_error: 1.2008 - mean_absolute_error: 0.7901\n",
      "Epoch 54/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.2263 - mean_squared_error: 1.2263 - mean_absolute_error: 0.7886\n",
      "Epoch 55/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.1092 - mean_squared_error: 1.1092 - mean_absolute_error: 0.7468\n",
      "Epoch 56/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.9708 - mean_squared_error: 0.9708 - mean_absolute_error: 0.6948\n",
      "Epoch 57/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9577 - mean_squared_error: 0.9577 - mean_absolute_error: 0.6893\n",
      "Epoch 58/100\n",
      "7982/7982 [==============================] - 0s 28us/step - loss: 0.8360 - mean_squared_error: 0.8360 - mean_absolute_error: 0.6266\n",
      "Epoch 59/100\n",
      "7982/7982 [==============================] - 0s 28us/step - loss: 1.4349 - mean_squared_error: 1.4349 - mean_absolute_error: 0.8693\n",
      "Epoch 60/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.2237 - mean_squared_error: 1.2237 - mean_absolute_error: 0.7835\n",
      "Epoch 61/100\n",
      "7982/7982 [==============================] - 0s 28us/step - loss: 0.9782 - mean_squared_error: 0.9782 - mean_absolute_error: 0.6926\n",
      "Epoch 62/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 0.9603 - mean_squared_error: 0.9603 - mean_absolute_error: 0.6857\n",
      "Epoch 63/100\n",
      "7982/7982 [==============================] - 0s 28us/step - loss: 1.0657 - mean_squared_error: 1.0657 - mean_absolute_error: 0.7367\n",
      "Epoch 64/100\n",
      "7982/7982 [==============================] - 0s 28us/step - loss: 1.2775 - mean_squared_error: 1.2775 - mean_absolute_error: 0.8116\n",
      "Epoch 65/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 0.8737 - mean_squared_error: 0.8737 - mean_absolute_error: 0.6445\n",
      "Epoch 66/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 0.8485 - mean_squared_error: 0.8485 - mean_absolute_error: 0.6343\n",
      "Epoch 67/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.2661 - mean_squared_error: 1.2661 - mean_absolute_error: 0.7957\n",
      "Epoch 68/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9949 - mean_squared_error: 0.9949 - mean_absolute_error: 0.7049\n",
      "Epoch 69/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 0.9902 - mean_squared_error: 0.9902 - mean_absolute_error: 0.6996\n",
      "Epoch 70/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 1.2128 - mean_squared_error: 1.2128 - mean_absolute_error: 0.7783\n",
      "Epoch 71/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9648 - mean_squared_error: 0.9648 - mean_absolute_error: 0.6860\n",
      "Epoch 72/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 1.1565 - mean_squared_error: 1.1565 - mean_absolute_error: 0.7511\n",
      "Epoch 73/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.0612 - mean_squared_error: 1.0612 - mean_absolute_error: 0.7272\n",
      "Epoch 74/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 0.9157 - mean_squared_error: 0.9157 - mean_absolute_error: 0.6673\n",
      "Epoch 75/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9556 - mean_squared_error: 0.9556 - mean_absolute_error: 0.6864\n",
      "Epoch 76/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 1.1181 - mean_squared_error: 1.1181 - mean_absolute_error: 0.7572\n",
      "Epoch 77/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 0.8732 - mean_squared_error: 0.8732 - mean_absolute_error: 0.6518\n",
      "Epoch 78/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 0.8512 - mean_squared_error: 0.8512 - mean_absolute_error: 0.6436\n",
      "Epoch 79/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 0.8499 - mean_squared_error: 0.8499 - mean_absolute_error: 0.6438\n",
      "Epoch 80/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 1.1060 - mean_squared_error: 1.1060 - mean_absolute_error: 0.7489\n",
      "Epoch 81/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 1.1177 - mean_squared_error: 1.1177 - mean_absolute_error: 0.7594\n",
      "Epoch 82/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 0.9528 - mean_squared_error: 0.9528 - mean_absolute_error: 0.6807\n",
      "Epoch 83/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 0.9721 - mean_squared_error: 0.9721 - mean_absolute_error: 0.6968\n",
      "Epoch 84/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 0.8523 - mean_squared_error: 0.8523 - mean_absolute_error: 0.6459\n",
      "Epoch 85/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 1.1869 - mean_squared_error: 1.1869 - mean_absolute_error: 0.7864\n",
      "Epoch 86/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9237 - mean_squared_error: 0.9237 - mean_absolute_error: 0.6844\n",
      "Epoch 87/100\n",
      "7982/7982 [==============================] - 0s 33us/step - loss: 0.9795 - mean_squared_error: 0.9795 - mean_absolute_error: 0.7034\n",
      "Epoch 88/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 0.9585 - mean_squared_error: 0.9585 - mean_absolute_error: 0.6910\n",
      "Epoch 89/100\n",
      "7982/7982 [==============================] - 0s 27us/step - loss: 0.8861 - mean_squared_error: 0.8861 - mean_absolute_error: 0.6636\n",
      "Epoch 90/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 0.8761 - mean_squared_error: 0.8761 - mean_absolute_error: 0.6576\n",
      "Epoch 91/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.0113 - mean_squared_error: 1.0113 - mean_absolute_error: 0.7185\n",
      "Epoch 92/100\n",
      "7982/7982 [==============================] - 0s 32us/step - loss: 0.8773 - mean_squared_error: 0.8773 - mean_absolute_error: 0.6616\n",
      "Epoch 93/100\n",
      "7982/7982 [==============================] - 0s 29us/step - loss: 0.9666 - mean_squared_error: 0.9666 - mean_absolute_error: 0.6932\n",
      "Epoch 94/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.3293 - mean_squared_error: 1.3293 - mean_absolute_error: 0.8293\n",
      "Epoch 95/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.8109 - mean_squared_error: 0.8109 - mean_absolute_error: 0.6227\n",
      "Epoch 96/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.0030 - mean_squared_error: 1.0030 - mean_absolute_error: 0.7174\n",
      "Epoch 97/100\n",
      "7982/7982 [==============================] - 0s 31us/step - loss: 0.9270 - mean_squared_error: 0.9270 - mean_absolute_error: 0.6848\n",
      "Epoch 98/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 1.0367 - mean_squared_error: 1.0367 - mean_absolute_error: 0.7153\n",
      "Epoch 99/100\n",
      "7982/7982 [==============================] - 0s 34us/step - loss: 0.8958 - mean_squared_error: 0.8958 - mean_absolute_error: 0.6750\n",
      "Epoch 100/100\n",
      "7982/7982 [==============================] - 0s 30us/step - loss: 0.9155 - mean_squared_error: 0.9155 - mean_absolute_error: 0.6726\n",
      "1996/1996 [==============================] - 0s 34us/step\n",
      "MSE: 1.5339680910110474, MAE: 1.013108730316162\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim = X.shape[1], activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer = 'adam', metrics=['mean_squared_error','mean_absolute_error'])\n",
    "model.fit(X, y, epochs = 1000, batch_size = 16)\n",
    "\n",
    "_, MSE, MAE = model.evaluate(Xt, yt)\n",
    "print('MSE: ' + str(MSE) + ', MAE: '+ str(MAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Bias of Smoothed Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE:  1.533967943681707\n",
      "Test MAE:  1.0131087721948757\n"
     ]
    }
   ],
   "source": [
    "test_predicted = model.predict(Xt)\n",
    "mse = mean_squared_error(yt, test_predicted)\n",
    "print(\"Test MSE: \",mse)\n",
    "mae = mean_absolute_error(yt, test_predicted)\n",
    "print(\"Test MAE: \",mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset to Visualize Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_data = pd.read_csv(\"census_block_loc.csv\")\n",
    "block_data = block_data.drop(columns=[\"BlockCode\",\"County\",\"State\"])\n",
    "block_data[\"Raw_Bias\"]=np.random.normal(scale=10,size=block_data.shape[0])\n",
    "block_data[\"County_Bias\"]=np.random.normal(scale=1,size=block_data.shape[0])\n",
    "block_data[\"Smoothed_Bias\"]=np.random.normal(scale=5,size=block_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put in visuals here from the dataset \"block_data\"\n",
    "#real data will be replaced when Erik figures out the proper way to do the smoothing\n",
    "\n",
    "import gmaps\n",
    "\n",
    "locations = sample[['latitude','longitude']]\n",
    "weights = sample['weight']\n",
    "\n",
    "coords = (40.75, -74.00)\n",
    "fig = gmaps.figure(center = coords, zoom_level = 12)\n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights))\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
